{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTl88k9F_jn_"
      },
      "source": [
        "## Weak law of large numbers\n",
        "Let $X_1, X_2, \\ldots, X_n$ be i.i.d. random variables with finite variance, i.e. $\\operatorname{Var}[X] < \\infty$, then\n",
        "\n",
        "$$\\frac{1}{n} \\sum_{i=1}^n X_i \\overset{p}{\\rightarrow} \\mathbb{E}[X], \\text{ as } n \\rightarrow \\infty $$\n",
        "More generally, for any $k$th moment, if $\\mathbb{E}[X^{k + 1}] < \\infty$, then\n",
        "\n",
        "$$\\frac{1}{n} \\sum_{i=1}^n X_i^k \\overset{p}{\\rightarrow} \\mathbb{E}[X^k], \\text{ as } n \\rightarrow \\infty $$\n",
        "\n",
        "Summary: as the size of samples increase towards infinity, sample features approach population corollaries.\n",
        "\n",
        "In the code that follows we're going to try to better understand this result through simulation. Run the code below to replicate the convergence plot from the slides. \n",
        "\n",
        "**Exercises:**\n",
        "\n",
        "1. Try modifying the simulation by increasing the maximum sample size, the number of simulated sequences, or the underlying population distribution. What do you notice?\n",
        "\n",
        "2. A bit more difficult: we also mentioned the WLLN applies to higher order moments. What happens if you look at the sample variance rather than the mean?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjTNCRJh9UOI"
      },
      "source": [
        "library(tidyverse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-T3OhTCRmU"
      },
      "source": [
        "N <- 200     # maximum sample size\n",
        "SIMS <- 5    # number of simulated convergence sequences\n",
        "\n",
        "# create simulation grid\n",
        "df <- expand_grid(\n",
        "  n = 1:N,\n",
        "  sim = 1:SIMS\n",
        ")\n",
        "\n",
        "# draw values from the normal(0, 1) distribution\n",
        "df$x <- rnorm(N * SIMS, mean = 0, sd = 1)\n",
        "\n",
        "# calculate running average\n",
        "df <- df %>% \n",
        "  group_by(sim) %>% \n",
        "  mutate(xbar = cumsum(x) / n)\n",
        "\n",
        "# plot\n",
        "ggplot(df) +\n",
        "    aes(n, xbar, color = factor(sim), group = factor(sim)) + \n",
        "    geom_line(size = 1.1) +\n",
        "    theme_bw(base_size = 16) +\n",
        "    theme(legend.position = 'none') + \n",
        "    scale_color_brewer(palette = \"Blues\") +\n",
        "    labs(\n",
        "      x = \"Sample size (n)\",\n",
        "      y = expression(bar(X)[n])\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiqBEoyEGO-u"
      },
      "source": [
        "## The Central Limit Theorem\n",
        "Let $X_1, X_2, \\ldots, X_n$ be i.i.d. random variables with finite mean, $\\mathbb{E}[X] = \\mu$, and variance, $\\operatorname{Var}[X] = \\sigma^2$, then\n",
        "\n",
        "$$ \\frac{\\sqrt{n}(\\bar{X} - \\mu)}{\\sigma} \\overset{d}{\\rightarrow} N(0, 1)$$\n",
        "or equivalently, \n",
        "\n",
        "$$ \\bar{X} \\overset{d}{\\rightarrow} N(\\mu, \\dfrac{\\sigma^2}{n})$$\n",
        "\n",
        "Summary: as the size of samples increase towards infinity, the distribution of the sample mean is approximately Normal.\n",
        "\n",
        "In the code that follows we're going to try to better understand this result through simulation. Run the code below to replicate the central limit theorem plot from the slides. \n",
        "\n",
        "**Exercises:**\n",
        "\n",
        "1. Try modifying the simulation by changing the sample sizes, the number of simulations, or the underlying population distribution. What do you notice?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VthJ64J_GacA"
      },
      "source": [
        "\n",
        "\n",
        "sims <- 10000       # number of simulations\n",
        "n <- c(10, 30, 100) # sample sizes\n",
        "\n",
        "df <- data.frame(\n",
        "  y = c(-3, 3)\n",
        ")\n",
        "\n",
        "# plot the population distribution\n",
        "g1 <- ggplot(df) +\n",
        "  aes(y) +\n",
        "  stat_function(\n",
        "    fun = dnorm,\n",
        "    n = 101,\n",
        "    args = list(mean = 0, sd = 1),\n",
        "    size = 1.1\n",
        "  ) +\n",
        "  geom_area(\n",
        "    stat = \"function\",\n",
        "    fun = dnorm,\n",
        "    fill = \"#23395b\",\n",
        "    xlim = c(-3, 3),\n",
        "    alpha = 0.5\n",
        "  ) +\n",
        "  theme_bw(base_size = 16) +\n",
        "  theme(legend.position = 'none') +\n",
        "  labs(\n",
        "    x = \"X\",\n",
        "    y = expression(Pr(X == x)))\n",
        "\n",
        "\n",
        "# create simulation grid\n",
        "df <- expand_grid(\n",
        "  n = n,\n",
        "  sim = 1:sims\n",
        ")\n",
        "\n",
        "# calculate the means of the sample\n",
        "df$xbar <- map_dbl(df$n, ~mean(rnorm(.x)))\n",
        "\n",
        "# create nicely formatted facet variable\n",
        "df$n <-\n",
        "  factor(x = df$n,\n",
        "         labels = c(\n",
        "           expression(bar(X)[10]),\n",
        "           expression(bar(X)[30]), \n",
        "           expression(bar(X)[100])\n",
        "          )\n",
        "        )\n",
        "\n",
        "# plot simulation results\n",
        "g2 <- ggplot(df) +\n",
        "    aes(xbar) + \n",
        "    facet_grid(~n, labeller = label_parsed) +\n",
        "    geom_histogram(bins = 100) +\n",
        "    theme_bw(base_size = 16) +\n",
        "    theme(legend.position = 'none') + \n",
        "    labs(\n",
        "      x = expression(bar(X)[n]),\n",
        "      y = \"count\"\n",
        "    )\n",
        "\n",
        "g1 \n",
        "g2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EnNzvf6O_iT"
      },
      "source": [
        "## P-values\n",
        "Let $\\widehat{\\theta}$ be an estimator of $\\theta$ and let $\\widehat{\\theta}^*$ be the observed value. \n",
        "\n",
        "Then a one-sided p-value is\n",
        "\n",
        "$$p = \\Pr\\left[\\widehat{\\theta} \\geq \\widehat{\\theta}^* \\mid \\theta = \\theta_0\\right] \\quad \\text{or} \\quad p = \\Pr\\left[\\widehat{\\theta} \\leq \\widehat{\\theta}^* \\mid \\theta = \\theta_0\\right]$$\n",
        "and a two-sided p-value is\n",
        "\n",
        "$$p = \\Pr\\left[|\\widehat{\\theta}| \\geq |\\widehat{\\theta}^*| \\mid \\theta = \\theta_0\\right]$$\n",
        "The probability of obtaining an estimate as extreme or more, **when the null is true**. \n",
        "\n",
        "## Normal approximation-based p-values\n",
        "Based on the **central limit theorem**, we know that the limit distribution of many estimators as $n \\rightarrow \\infty$ is Normal. We can use this fact to calculate an approximate p-value based on the Normal distribution. \n",
        "\n",
        "For a one-sided test\n",
        "$$p = \\Phi\\left(\\frac{\\widehat{\\theta}^* - \\theta_0}{\\sqrt{\\operatorname{Var}[\\widehat{\\theta}]}}\\right)  \\quad \\text{ or } \\quad p = \\Phi\\left(\\frac{\\widehat{\\theta}^* - \\theta_0}{\\sqrt{\\operatorname{Var}[\\widehat{\\theta}]}}\\right)$$\n",
        "\n",
        "For a two-sided test\n",
        "$$p = 2\\left(1 - \\Phi\\left(\\frac{|\\widehat{\\theta}^* - \\theta_0|}{\\sqrt{\\operatorname{Var}[\\widehat{\\theta}]}}\\right)\\right)$$\n",
        "Where $\\Phi$ is shorthand for the CDF of the standard normal distribution (i.e. $N(0, 1)$).\n",
        "\n",
        "In the slides, we also saw that we can calculate a p-value for any hypothesized value of $\\theta$ and use this to create a p-value function. In this simulation, we'll show you an example of this.\n",
        "\n",
        "\n",
        "**Exercises:**\n",
        "\n",
        "1. Try modifying the simulation by changing the sample size or the underlying population distribution. What do you notice?\n",
        "\n",
        "2. If you change the distribution to Cauchy, what happens? Does this make sense based on what you know about the Cauchy?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc_041iDRGHf"
      },
      "source": [
        "n <- 100                       # the sample size\n",
        "hyp <- seq(10, 90, by = 0.01)  # hypothesis values to search over\n",
        "\n",
        "# draw data from population distribution\n",
        "x <- rnorm(n, mean = 50, sd = 40)\n",
        "\n",
        "# calculate sample mean and estimate sampling variance\n",
        "xbar <- mean(x)\n",
        "sampling_variance <- sd(x) / sqrt(n)\n",
        "\n",
        "\n",
        "df <- data.frame(\n",
        "  hyp = hyp,\n",
        "  p = 2 * (1 - pnorm(abs(xbar - hyp)/sampling_variance))\n",
        ")\n",
        "\n",
        "\n",
        "ggplot(df) +\n",
        "    aes(hyp, p) + \n",
        "    geom_line() + \n",
        "    geom_vline(xintercept = xbar, linetype = \"dotted\") +\n",
        "    geom_vline(xintercept = 50) +\n",
        "    theme_bw(base_size = 16) +\n",
        "    theme(legend.position = 'none') + \n",
        "    labs(\n",
        "      x = expression(theta[0]),\n",
        "      y = \"p-value\"\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DRhuC4zINYk"
      },
      "source": [
        "## Confidence Intervals\n",
        "\n",
        "A valid 95% confidence interval for parameter $\\theta$ is a random interval $CI_{95}(\\theta) = (X_{lower}, X_{upper})$ such that \n",
        "\n",
        "$$\\Pr[\\theta \\in CI_{95}(\\theta)] \\geq 0.95$$\n",
        "\n",
        "Based on the **central limit theorem**, we know that the limit distribution of the sample mean as $n \\rightarrow \\infty$ is Normal. We can use this fact to derive an approximate 95% confidence interval based on the Normal distribution: \n",
        "\n",
        "$$CI_{95} = (\\widehat{\\theta} - Z_{0.025} \\cdot \\widehat{SE}(\\widehat{\\theta}), \\widehat{\\theta} + Z_{0.975} \\cdot \\widehat{SE}(\\widehat{\\theta}))$$\n",
        "\n",
        "where $\\widehat{\\theta} = \\frac{1}{n}\\sum_i X_i$ and $\\widehat{SE}(\\widehat{\\theta}) = \\sqrt{\\frac{\\sigma^2}{n}}$.\n",
        "\n",
        "In the code that follows we're going to try to better understand this result through simulation. Run the code below to replicate the confidence interval plot from the slides.\n",
        "\n",
        "**Exercises:**\n",
        "\n",
        "1. Try modifying the simulation by changing the sample sizes, the number of simulations, or the underlying population distribution. What do you notice?\n",
        "\n",
        "2. To make the plot a little nicer can you modify it so the estimates are in order from lowest to highest?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0_c2frNIMiK"
      },
      "source": [
        "SIMS <- 100\n",
        "n <- 50\n",
        "truth <- 0\n",
        "\n",
        "df <- tibble(\n",
        "  sim = 1:SIMS,\n",
        "  y = map(1:SIMS, ~rnorm(n)),\n",
        "  mean = map_dbl(y, ~mean(.x)),\n",
        "  sd = map_dbl(y, ~sd(.x)),\n",
        "  upr = mean + 1.96 * sd / sqrt(n),\n",
        "  lwr = mean - 1.96 * sd / sqrt(n),\n",
        "  covers = as.numeric(lwr <= truth & truth <= upr)\n",
        ")\n",
        "\n",
        "ggplot(df) +\n",
        "    aes(x = sim, ymin = lwr, ymax = upr, y = mean, color = factor(covers)) +\n",
        "    geom_hline(yintercept = 0, linetype = \"dashed\") +\n",
        "    geom_pointrange() +\n",
        "    scale_color_manual(name = \"Covers\", labels = c(\"No\", \"Yes\"), values = c(\"red\", \"black\")) +\n",
        "    theme_bw(base_size = 16) +\n",
        "    labs(x = \"Sample\", \n",
        "         y = expression(widehat(theta))) +\n",
        "    coord_flip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFnp61E2KH7k"
      },
      "source": [
        "empirical_coverage <- mean(df$covers)\n",
        "print(empirical_coverage)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}